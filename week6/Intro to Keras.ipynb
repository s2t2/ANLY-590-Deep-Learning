{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Keras\n",
    "- You'll find that our programming style is very similar to how Keras has implemented networks and training\n",
    "- We'll introduce a number of keys concepts through examples\n",
    "- As the course progresses new methods and api calls will be introduced\n",
    "- We'll review the MNIST dataset and reconstruct our previously built networks using the Keras api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data: Pipeline\n",
    "- Previously we saw how to download data using `keras`\n",
    "- Our data pipeline will follow the steps:\n",
    "    1. Download data\n",
    "    2. Check size of data\n",
    "    3. Convert numeric values to categories using one-hot-encoding\n",
    "    4. Convert data to `float32`\n",
    "    5. Reshape data \n",
    "    6. Rescale the data (so that the values are between 0 and 1)\n",
    "- The aforementioned steps are common in most pre-processing steps before the data can be used\n",
    "- As we move forward in the course a similar approach will be used with other datasets\n",
    "- Note:\n",
    "    - When saving a model's weights, `tf.keras defaults` to the checkpoint format\n",
    "    - Pass `save_format='h5'` to use `HDF5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kornraphop/.pyenv/versions/3.6.2/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n",
      "The number of training examples: (60000, 28, 28)\n",
      "The number of test examples: (10000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1092da6d8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Load data ---\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# --- Checking size of MNIST Set ---\n",
    "print(\"The number of training examples: \"+str(X_train.shape))\n",
    "print(\"The number of test examples: \"+str(X_test.shape))\n",
    "\n",
    "# --- Y_train to categorical ---\n",
    "# Need to change these from numbers to categories\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# --- Plotting data ---\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(X_train[0])\n",
    "\n",
    "# --- Converting type and Normalize Values ---\n",
    "n_train, l_train, w_train = X_train.shape\n",
    "n_test, l_test, w_test = X_test.shape\n",
    "X_train = X_train.astype('float32').reshape(n_train, l_train * w_train)\n",
    "X_test = X_test.astype('float32').reshape(n_test, l_test * w_test)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# -- Split Train/Validation set --\n",
    "n_val = 3000\n",
    "X_val = X_train[:n_val,:]; X_train = X_train[n_val:,:]\n",
    "Y_val = Y_train[:n_val,:]; Y_train = Y_train[n_val:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple model\n",
    "- Keras has two ways to build models:\n",
    "    1. Sequential - stacked layers\n",
    "    2. Functional - multi input/output, recursive cells\n",
    "- Over the next couple of sessions we'll use the sequential model\n",
    "- The functional approach will come into play in the later parts of the session\n",
    "\n",
    "#### Connecting networks\n",
    "- Previously the input-output matrix shape needed to be specified for each layer \n",
    "- Being explicit about shapes is always a good idea since it forces you to think the architecture of the model\n",
    "- `keras` however provides automatic shape inference \n",
    "- Only the first layer's shape needs to be specified, it can be done in several ways:\n",
    "    - `input_shape` - tuple of integers\n",
    "    - `input_dim` - for 2D layers\n",
    "    - `input_dim`, `input_length` - for 3D temporal layers\n",
    "    - *`batch_size` - may be included as an optional parameter with shape, i.e. `batch_size=32, input_shape=(6,8)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --- Import Librarires ----\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# --- Setting up a Sigmoid Sequential Model ---\n",
    "# Initialize model\n",
    "model = Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "model.add(Dense(64, input_shape = (784,), activation='sigmoid'))\n",
    "# Add another:\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "# Add a softmax layer with 10 output units:\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# Check model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-231b805e7900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m            \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m            \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m            rankdir='TB') #'TB' = vertical plot, 'LR' = Horizontal plot\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# -- Libraries for plotting inline --\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         raise ImportError(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "# --- Visualizing the model ----\n",
    "# Note: pydot and graphviz required to run command\n",
    "from keras.utils import plot_model\n",
    "# -- Save model to file --\n",
    "plot_model(model, \n",
    "           to_file='/home/joshuah/Desktop/github/guLectureNotes/model.png',\n",
    "           show_shapes=True,\n",
    "           show_layer_names=True,\n",
    "           rankdir='TB') #'TB' = vertical plot, 'LR' = Horizontal plot\n",
    "\n",
    "# -- Libraries for plotting inline --\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the layers\n",
    "- Different layer types are available along with common constructor parameters:\n",
    "- Among the most important is `activation`:\n",
    "    - `sigmoid`,`softmax`, `tanh`, `hard_sigmoid`\n",
    "    - `relu`, `elu`,`selu`\n",
    "    - `linear`\n",
    "    - `softsign`, `softplus`\n",
    "- Advanced activation layers such as `PReLu` and `LeakyReLu` are available through `keras.layers.advanced_activations`\n",
    "- To read more vist the [Advanced Activation Layer](https://keras.io/layers/advanced-activations/) documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --- Setting up a ReLu Sequential Model ---\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(64,activation='relu', input_shape = (784,)))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Additional parameters when configuring layers include:    \n",
    "    - `use_bias`: boolean indication if bias should be used\n",
    "    - `kernel_initialize` and `bias_initialize`: initializes weight values `\"Glorot uniform\"` is the default\n",
    "    - `kernel_regularizer` and `bias_regularizer`: regularization schemes applied to different layers\n",
    "    - `kernel_constraint` and `bias_constraint` : construction applied to the kernel weights matrix and bias vector respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 64)                28928     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 42,058\n",
      "Trainable params: 42,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --- Setting up a ReLu Sequential Model with Regularizers ---\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.initializers import constant\n",
    "# -- Model build --\n",
    "model3 = Sequential()\n",
    "# ReLu L1 regularization of factor 0.01 applied to the kernel matrix:\n",
    "model3.add(Dense(64, \n",
    "                 activation = 'relu',\n",
    "                 kernel_regularizer=l1(0.01),\n",
    "                 input_shape = (451,)))\n",
    "\n",
    "# ReLu with L2 regularization of factor 0.01 applied to the bias vector:\n",
    "model3.add(Dense(64, bias_regularizer=l2(0.01)))\n",
    "# Layer with a kernel initialized to a random orthogonal matrix:\n",
    "model3.add(Dense(64, kernel_initializer='orthogonal'))\n",
    "# Layer with a bias vector initialized to 2.0s:\n",
    "model3.add(Dense(64, bias_initializer=constant(2.0)))\n",
    "# Sigmoid output unit\n",
    "model3.add(Dense(10, activation='softmax'))\n",
    "# Model summary\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling a model\n",
    "- Once the model is constructed it needs to be __compiled__\n",
    "- Compiling a model means telling the computer to lazily evaluate:\n",
    "    - The architecture\n",
    "    - Optimization method\n",
    "    - Number of batches\n",
    "    - Metrics to follow\n",
    "- Depending on the type of problem the appropriate parameters will need to be selected\n",
    "- Inputs for the compilation method include:\n",
    "    - `optimizer`: the type of optimization method used during training, common parameters include: `AdamOptimizer`, `RMSPropOptimizer`, `GradientDescentOptimizer`\n",
    "    - ` loss`: function to minimize during optimization\n",
    "        - `mse`: regression\n",
    "        - `categorical_crossentropy`: multiclass-classificaation\n",
    "        - `binary_crossentropy`: vanilla classification\n",
    "    - `mae`: metrics used to monitor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There might be times when you want to setup a __custom loss metric__:\n",
    "```{python}\n",
    "# --- For custom metrics ---\n",
    "# Import keras backend\n",
    "import keras.backend as K\n",
    "# - Customer metric - \n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "# Compile model with custom metric\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', mean_pred])\n",
    "```\n",
    "\n",
    "- Setting the __optimization hyperparameters __ can also be done before compiling:\n",
    "```\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "- There are many __Optimizers__ to choose from, read more at:\n",
    " * https://keras.io/optimizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating\n",
    "- Use the function `fit` to train the model\n",
    "- Several options exist within the `fit` call:\n",
    "    - `batch_size`: \n",
    "        - The model slices the data into smaller batches and iterates over these batches during training\n",
    "        - This integer specifies the size of each batch\n",
    "        - Recall smaller batch sizes occupy less memory and are closer to online gradient descent\n",
    "    - `epochs` : number of times to iterate over the data\n",
    "    - `shuffle`: shuffle the data\n",
    "    - `validation_data`: validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57000 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "57000/57000 [==============================] - 2s 40us/step - loss: 0.6889 - acc: 0.8397 - val_loss: 0.2953 - val_acc: 0.9143\n",
      "Epoch 2/3\n",
      "57000/57000 [==============================] - 2s 34us/step - loss: 0.2552 - acc: 0.9258 - val_loss: 0.2182 - val_acc: 0.9343\n",
      "Epoch 3/3\n",
      "57000/57000 [==============================] - 2s 33us/step - loss: 0.1965 - acc: 0.9424 - val_loss: 0.1802 - val_acc: 0.9460\n",
      "Batch size 64: takes 6.246483087539673 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# Train neural network\n",
    "from time import time\n",
    "start_time = time()\n",
    "history_64 = model.fit(X_train, Y_train,\n",
    "              batch_size=64, # Batch sizes - runtime scales ~linearly\n",
    "              verbose = 1,\n",
    "              epochs=3, \n",
    "              shuffle=False,\n",
    "              validation_data=(X_val, Y_val))\n",
    "time_to_complete = time()- start_time\n",
    "print(\"Batch size 64: takes {} seconds to complete\".format(time_to_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129d8ec88>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Plotting data/training model ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get loss function\n",
    "loss = history_64.history['loss']\n",
    "val_loss = history_64.history['val_loss']\n",
    "\n",
    "# Setup grid for plotting\n",
    "epochs =range(1, len(loss) + 1)\n",
    "# Plot\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model performance\n",
    "- Several options exist for evaluate model performance\n",
    "- We can predict the output values using:\n",
    "    - `model.predict` and \n",
    "    - `model.predict_classes` - wrapper for model.predict for classes\n",
    "- Alternatively we can use the `model.evaluate` function and score it's performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 18us/step\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluation metrics ---\n",
    "import numpy as np\n",
    "# Evaluation metrics\n",
    "score = model.evaluate(X_test, Y_test, batch_size=64)\n",
    "predict_probabilities = model.predict(X_test, batch_size = 64)\n",
    "predict_classes = model.predict_classes(X_test, batch_size = 64)\n",
    "\n",
    "# -- Check to show predict and predict_classes are the same --\n",
    "classes_from_probs =  np.argmax(predict_probabilities, axis = 1)\n",
    "assert(np.alltrue(predict_classes == classes_from_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On the test set:\n",
      "The mean validation loss is: 0.1817934366106987,\n",
      "The mean accuracy score is: 0.9459\n",
      "The first 5 predict probabilites are: [7 2 1 0 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "\"\"\"\n",
    "On the test set:\n",
    "The mean validation loss is: {mean_test_loss},\n",
    "The mean accuracy score is: {mean_test_accuracy}\n",
    "The first 5 predict probabilites are: {pred_class}\n",
    "\"\"\".format(mean_test_loss = score[0],\n",
    "           mean_test_accuracy = score[1],\n",
    "          pred_class = predict_classes[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization and parameter tuning\n",
    "- Previously we saw that L1/L2 regularization could easily be incorporated in the activation unit\n",
    "- Other regularizers are added to the network slightly differently\n",
    "- Below we present a network with dropout, custom RMSprop parameters and early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12a081470>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Setting up a ReLu Sequential Model with Dropout  ---\n",
    "from keras.layers import Dropout\n",
    "# Build model\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(64,activation='relu', input_shape = (784,)))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(Dense(64, activation='relu'))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(Dense(10, activation='softmax'))\n",
    "model4.summary()\n",
    "\n",
    "# --- Modifying training parameters ---\n",
    "from keras.optimizers import RMSprop\n",
    "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
    "model4.compile(loss='categorical_crossentropy', \n",
    "              optimizer=opt, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# --- Early stopping ---\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "model4.fit(X_train, Y_train,\n",
    "               batch_size=64, \n",
    "               epochs= 3,\n",
    "               verbose = 0,\n",
    "               validation_data=(X_val, Y_val),\n",
    "               callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving/reloading models\n",
    "- Saving a model in `HDF5` format allows you to load\n",
    "    - The architecture of the model, allowing to re-create the model\n",
    "    - Weights\n",
    "    - Training configuration (loss, optimizer)\n",
    "    - State of the optimizer (you can resume training where you left off)\n",
    "- To save a model in `HDF5` format you will need to install `h5py`\n",
    "- Models can be saved by architecture and weight components in different formats (`YAML`, `json`)\n",
    "- Componentizing this approach offers certain benefits:\n",
    "    - Saving architecture's rather than all the weights frees up space\n",
    "    - Saving only weights makes sense if you already have the model\n",
    "    - Saving models in different formats allows you to port them to other `tensorflow` and `keras` ports/backends such as `tensorflow.js`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setting up directory --\n",
    "import os\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# --- Saving full models in HDF5 Format ---\n",
    "from keras.models import load_model\n",
    "# Save model in HDF5 fomrat\n",
    "model.save(os.path.join(base_dir,'my_model.h5')) \n",
    "# Deletes existing model\n",
    "del model \n",
    "\n",
    "# --- Load model ---\n",
    "# Returns a compiled model identical to the previous one\n",
    "model = load_model(os.path.join(base_dir,'my_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Saving only a models architecture ---\n",
    "# Convert model to JSON\n",
    "json_string = model.to_json()\n",
    "\n",
    "# Convert model to YAML\n",
    "yaml_string = model.to_yaml()\n",
    "\n",
    "# --- Reconstructing model from different formats ---\n",
    "from keras.models import model_from_json\n",
    "from keras.models import model_from_yaml\n",
    "# JSON model reload\n",
    "model_json = model_from_json(json_string)\n",
    "# YAML model load\n",
    "model_yaml = model_from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Saving/Loading Model's Weights (only) ---\n",
    "# Vanilla save/load\n",
    "model.save_weights(os.path.join(base_dir,'my_model_weights.h5'))\n",
    "model.load_weights(os.path.join(base_dir,'my_model_weights.h5'))\n",
    "\n",
    "# --- Transfer learning approach to re-load ---\n",
    "# When we load weights into a different \n",
    "# architeture with some common layers\n",
    "model.load_weights(os.path.join(base_dir,'my_model_weights.h5'), \n",
    "                   by_name=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "- There are many functions which we have not covered however this should be enough to get you started\n",
    "- Some other utility functions include `shape`,`configuration` and `weights` which are applied to the model, see below for some examples:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 10)\n",
      "[{'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'batch_input_shape': (None, 784), 'dtype': 'float32', 'units': 64, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'units': 64, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_3', 'trainable': True, 'units': 10, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Utility functions ---\n",
    "print(model.output_shape)\n",
    "print(model.get_config())\n",
    "model.get_weights()\n",
    "model.layers\n",
    "model.layers[0].input_shape\n",
    "model.layers[0].output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional API\n",
    "Most of the network architectures we will explore can be implemented with a sequential stack of layers. Indeed, all deep feedforward networks, convolutional networks, and recurrent networks are of this type. However, it might be the case that network architecture requires \"branching\" of data flows and layers. An example might be a multi-task network with a set of shared weights. To solve problems of this type, Keras has an entirely different API that is a bit more flexible. \n",
    "\n",
    "You may never need this kind of network, but you can read more about the Functional API here:\n",
    "\n",
    "* https://keras.io/getting-started/functional-api-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - MNIST\n",
    "Load the MNIST image dataset. Use plt.imshow() to visualize a few of the images.\n",
    "\n",
    "1. Build a simple linear classifier to predict which image class is represented in each image. This should be equivalent to multi-class logistic regression and this model will serve as our baselne.\n",
    "\n",
    "2. Build a deeper network with ReLU activations.\n",
    "\n",
    "3. Experiment with using different optimizers: read the documentation for `adam` and `rmsprop` to understand and modify their hyperparameters. \n",
    "\n",
    "4. Includer regularizers in different parts of your network. Use a train/test split to evaluate the imact of regularizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Bullseye\n",
    "Recall the Bullseye (or Donut) type of dataset: a binary classification problem where one of the classes entirely surrounds the other. The simplest version of this looks like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x12a8b3780>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a834c18>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.random.uniform(-5, 5, 500)\n",
    "Y = np.zeros(len(X))\n",
    "Y[(X<1) & (X>-1)] = 1\n",
    "plt.scatter(X, np.ones(len(X)), c=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Build\n",
    "Build a network to solve this problem. What is the simplest network you can find that solves it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_67 (Dense)             (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 377\n",
      "Trainable params: 377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --- Setting up a Sigmoid Sequential Model ---\n",
    "# Initialize model\n",
    "model = Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "model.add(Dense(16, input_shape = (1,), activation='relu'))\n",
    "# Add another:\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "# Add a softmax layer with 10 output units:\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Check model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11dd769e8>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y,\n",
    "           batch_size=5, \n",
    "           epochs= 3,\n",
    "           verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 752us/step\n",
      "Loss: 0.08602361287825261\n",
      "Accuracy: 0.976\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X, Y, batch_size=1)\n",
    "print(\"Loss: \" + str(score[0]))\n",
    "print(\"Accuracy: \" + str(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "We can examine the activations (or outputs) of any layer of a Keras model. For us, it will be useful to visualize the representation that our model has created in order to solve this problem. Recall, the dataset in the original 1-Dimensional space is not separable. So it is interesting to examine what new representation our network has learned so that the data classes become separable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This utility allows us to create a function for extracting the outputs of any layer in a Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def extract_layer_output(model, layer_num):\n",
    "    return K.function([model.input] + [K.learning_phase()], [model.layers[layer_num].output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model with only a single hidden layer and only 2 nodes in that hidden layer. Fit this model has been fit to the data. When we pass an observation through this network (forward pass) we can think outputs of the hidden layer as a representation of the observation, but in a new coordinate system that our network has learned. \n",
    "\n",
    "Let's calculate the activations at the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "`inputs` should be a list or tuple.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-77dcb8781ac5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mH1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_layer_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# layer 0 is the first hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mH1_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# what is the shape of the output matrix here?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2468\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2470\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2471\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: `inputs` should be a list or tuple."
     ]
    }
   ],
   "source": [
    "H1 = extract_layer_output(model, 0) # layer 0 is the first hidden layer\n",
    "H1_output = H1(X[:,np.newaxis]) # what is the shape of the output matrix here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a visualization of the representation in the 2D hidden layer. Consider using a scatterplot and coloring each datapoint accoring to its class. Comment on this representation and why it allows us to solve the bullseye problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this process with the output layer. Make a visualization of X versus the final output Y, and color by class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3  - XOR\n",
    "We'll revisit the Scattered XOR dataset. Build any network to solve this classification problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(low=-2,high=2,size=1000).reshape((500,2))\n",
    "y=np.zeros(500)\n",
    "y[np.logical_and(x[:,0]>0 , x[:,1]>0)]=1\n",
    "y[np.logical_and(x[:,0]<0 , x[:,1]<0)]=1\n",
    "\n",
    "plt.scatter(x[:,0], x[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Experiment with networks of different capacity to try to find the \"simplest\" model that can solve this problem. \n",
    "\n",
    "2. Now do the opposite - build a deep and complex network to solve this problem. Which one seems easier to fit? \n",
    "\n",
    "3. Experiment with using different optimizers: read the documentation for `adam` and `rmsprop` to understand and modify their hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
